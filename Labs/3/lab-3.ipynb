{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratory 3: Partially observable Markov decision problems\n",
    "\n",
    "In the end of the lab, you should export the notebook to a Python script (``File >> Download as >> Python (.py)``). Make sure that the resulting script includes all code written in the tasks marked as \"**Activity n. N**\", together with any replies to specific questions posed. Your file should be named `padi-labKK-groupXXX.py`, where `KK` corresponds to the lab number and the `XXX` corresponds to your group number. Similarly, your homework should consist of a single pdf file named `padi-hwKK-groupXXX.pdf`. You should create a zip file with the lab and homework files and submit it in Fenix **at most 30 minutes after your lab is over**.\n",
    "\n",
    "Make sure to strictly respect the specifications in each activity, in terms of the intended inputs, outputs and naming conventions.\n",
    "\n",
    "In particular, after completing the activities you should be able to replicate the examples provided (although this, in itself, is no guarantee that the activities are correctly completed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The POMDP model\n",
    "\n",
    "Consider once again the garbage collection problem described in the homework and for which you wrote a partially observable Markov decision problem model. In this lab, you will consider a larger version of that same problem, described by the diagram:\n",
    "\n",
    "<img src=\"garbage-big.png\">\n",
    "\n",
    "Recall that the POMDP should describe the decision-making process of the truck driver. In the above domain,\n",
    "\n",
    "* At any time step, garbage is _at most_ in one of the cells marked with a garbage bin. \n",
    "* When the garbage truck picks up the garbage from one of the bins, it becomes ``loaded``. \n",
    "* While the truck is loaded, no garbage appears in any of the marked locations.\n",
    "* The driver has six actions available: `Up`, `Down`, `Left`, `Right`, `Pick`, and `Drop`. \n",
    "* Each movement action moves the truck to the adjacent stop in the corresponding direction, if there is one. Otherwise, it has no effect. \n",
    "* The `Pick` action succeeds when the truck is in a location with garbage. In that case, the truck becomes \"loaded\".\n",
    "* The `Drop` action succeeds when the loaded truck is at the recycling plant. After a successful drop, the truck becomes empty, and garbage may now appear in any of the marked cells with a total probability of 0.3.\n",
    "* The driver cannot observe whether there is garbage in any of these locations unless if it goes there.\n",
    "\n",
    "In this lab you will use a POMDP based on the aforementioned domain and investigate how to simulate a partially observable Markov decision problem and track its state. You will also compare different MDP heuristics with the optimal POMDP solution.\n",
    "\n",
    "**Throughout the lab, unless if stated otherwise, use $\\gamma=0.99$.**\n",
    "\n",
    "$$\\diamond$$\n",
    "\n",
    "In this first activity, you will implement an POMDP model in Python. You will start by loading the POMDP information from a `numpy` binary file, using the `numpy` function `load`. The file contains the list of states, actions, observations, transition probability matrices, observation probability matrices, and cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "Write a function named `load_pomdp` that receives, as input, a string corresponding to the name of the file with the POMDP information, and a real number $\\gamma$ between $0$ and $1$. The loaded file contains 6 arrays:\n",
    "\n",
    "* An array `X` that contains all the states in the POMDP, represented as strings. In the garbage collection environment above, for example, there is a total of 462 states, each describing the location of the truck in the environment, the location of the garbage (or `None` if no garbage exists in the environment), and whether the truck is `loaded` or `empty`. Each state is, therefore, a string of the form `\"(p, g, t)\"`, where:\n",
    "    * `p` is one of `0`, ..., `32`, indicating the location of the truck;\n",
    "    * `g` is either `None` or one of `1`, `9`, `10`, `11`, `18`, `19`, `20`, `21`, `23`, `27`, `28`, `29`, indicating that no garbage exists (`None`), or that there is garbage in one of the listed stops;\n",
    "    * `t` is either `empty` or `loaded`, indicating whether the truck is loaded or not.\n",
    "* An array `A` that contains all the actions in the POMDP, also represented as strings. In the garbage collection environment above, for example, each action is represented as a string `\"Up\"`, `\"Down\"`, `\"Left\"`, `\"Right\"`, `\"Pick\"`, and `\"Drop\"`.\n",
    "* An array `Z` that contains all the observations in the POMDP, also represented as strings. In the garbage collection environment above, for example, there is a total of 78 observations, each describing the location of truck in the environment, whether it is loaded or empty, and whether it sees garbage in its current location. This means that the strings describing the observations take the form `\"(p, g, t)\"`, where:\n",
    "    * `p` is one of `0`, ..., `32`, indicating the location of the truck;\n",
    "    * `g` is either `no garbage` or `full` indicating, respectively, that the driver sees no garbage (either because there is no garbage bin in the current location or because the one in it is empty) or sees a full garbage bin;\n",
    "    * `t` is either `empty` or `loaded`, indicating whether the truck is loaded or not.\n",
    "* An array `P` containing `len(A)` subarrays, each with dimension `len(X)` &times; `len(X)` and  corresponding to the transition probability matrix for one action.\n",
    "* An array `O` containing `len(A)` subarrays, each with dimension `len(X)` &times; `len(Z)` and  corresponding to the observation probability matrix for one action.\n",
    "* An array `c` with dimension `len(X)` &times; `len(A)` containing the cost function for the POMDP.\n",
    "\n",
    "Your function should create the POMDP as a tuple `(X, A, Z, (Pa, a = 0, ..., len(A)), (Oa, a = 0, ..., len(A)), c, g)`, where `X` is a tuple containing the states in the POMDP represented as strings (see above), `A` is a tuple containing the actions in the POMDP represented as strings (see above), `Z` is a tuple containing the observations in the POMDP represented as strings (see above), `P` is a tuple with `len(A)` elements, where `P[a]` is an `np.array` corresponding to the transition probability matrix for action `a`, `O` is a tuple with `len(A)` elements, where `O[a]` is an `np.array` corresponding to the observation probability matrix for action `a`, `c` is an `np.array` corresponding to the cost function for the POMDP, and `g` is a float, corresponding to the discount and provided as the argument $\\gamma$ of your function. Your function should return the POMDP tuple.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T14:09:53.429443Z",
     "start_time": "2022-04-03T14:09:53.210129Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= State space (462 states) =\n",
      "\n",
      "States:\n",
      "(0, None, empty)\n",
      "(0, 1, empty)\n",
      "(0, 9, empty)\n",
      "(0, 10, empty)\n",
      "(0, 11, empty)\n",
      "(0, 18, empty)\n",
      "(0, 19, empty)\n",
      "(0, 20, empty)\n",
      "(0, 21, empty)\n",
      "(0, 23, empty)\n",
      "...\n",
      "\n",
      "Random state: s = (7, 28, empty)\n",
      "\n",
      "Last state: (32, None, loaded)\n",
      "hello1\n",
      "= Action space (6 actions) =\n",
      "Up\n",
      "Down\n",
      "Left\n",
      "Right\n",
      "Pick\n",
      "Drop\n",
      "hello2\n",
      "\n",
      "Random action: a = Right\n",
      "= Observation space (78 observations) =\n",
      "\n",
      "Observations:\n",
      "(8, no garbage, empty)\n",
      "(31, no garbage, loaded)\n",
      "(27, full, empty)\n",
      "(9, no garbage, loaded)\n",
      "(2, no garbage, loaded)\n",
      "(29, full, empty)\n",
      "(4, no garbage, empty)\n",
      "(23, no garbage, empty)\n",
      "(23, full, empty)\n",
      "(11, no garbage, loaded)\n",
      "...\n",
      "\n",
      "Random observation: z = (0, no garbage, loaded)\n",
      "\n",
      "Last observation: (12, no garbage, loaded)\n",
      "\n",
      "= Transition probabilities =\n",
      "\n",
      "Transition probability matrix dimensions (action Up): (462, 462)\n",
      "Dimensions add up for action \"Up\"? True\n",
      "\n",
      "Transition probability matrix dimensions (action Down): (462, 462)\n",
      "Dimensions add up for action \"Down\"? True\n",
      "\n",
      "Transition probability matrix dimensions (action Left): (462, 462)\n",
      "Dimensions add up for action \"Left\"? True\n",
      "\n",
      "Transition probability matrix dimensions (action Right): (462, 462)\n",
      "Dimensions add up for action \"Right\"? True\n",
      "\n",
      "Transition probability matrix dimensions (action Pick): (462, 462)\n",
      "Dimensions add up for action \"Pick\"? True\n",
      "\n",
      "Transition probability matrix dimensions (action Drop): (462, 462)\n",
      "Dimensions add up for action \"Drop\"? True\n",
      "\n",
      "State-action pair ((7, 28, empty), Right) transitions to state(s)\n",
      "s' in ['(8, 28, empty)']\n",
      "\n",
      "= Observation probabilities =\n",
      "\n",
      "Observation probability matrix dimensions (action Up): (462, 78)\n",
      "Dimensions add up for action \"Up\"? True\n",
      "\n",
      "Observation probability matrix dimensions (action Down): (462, 78)\n",
      "Dimensions add up for action \"Down\"? True\n",
      "\n",
      "Observation probability matrix dimensions (action Left): (462, 78)\n",
      "Dimensions add up for action \"Left\"? True\n",
      "\n",
      "Observation probability matrix dimensions (action Right): (462, 78)\n",
      "Dimensions add up for action \"Right\"? True\n",
      "\n",
      "Observation probability matrix dimensions (action Pick): (462, 78)\n",
      "Dimensions add up for action \"Pick\"? True\n",
      "\n",
      "Observation probability matrix dimensions (action Drop): (462, 78)\n",
      "Dimensions add up for action \"Drop\"? True\n",
      "\n",
      "State-action pair ((7, 28, empty), Right) yields observation(s)\n",
      "z in ['(7, no garbage, empty)']\n",
      "\n",
      "= Costs =\n",
      "\n",
      "Cost for the state-action pair ((7, 28, empty), Right):\n",
      "c(s, a) = 0.501\n",
      "\n",
      "= Discount =\n",
      "\n",
      "gamma = 0.99\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Insert your code here.\n",
    "def load_pomdp(filename, gamma):\n",
    "    \n",
    "    file = np.load(filename)\n",
    "\n",
    "    X = file[\"X\"] # States\n",
    "    A = file[\"A\"] # Actions\n",
    "    Z = file[\"Z\"] # Observations\n",
    "    P = file[\"P\"] # Transition probability matrix\n",
    "    O = file[\"O\"] # Observation probability matrix\n",
    "    c = file[\"c\"] # Cost function\n",
    "\n",
    "    # Define the MDP\n",
    "    M = (X, A, Z, P, O, c, gamma)\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "import numpy.random as rand\n",
    "\n",
    "M = load_pomdp('garbage-big.npz', 0.99)\n",
    "\n",
    "rand.seed(42)\n",
    "\n",
    "# States\n",
    "print('= State space (%i states) =' % len(M[0]))\n",
    "print('\\nStates:')\n",
    "for i in range(min(10, len(M[0]))):\n",
    "    print(M[0][i]) \n",
    "\n",
    "print('...')\n",
    "\n",
    "# Random state\n",
    "s = rand.randint(len(M[0]))\n",
    "print('\\nRandom state: s =', M[0][s])\n",
    "\n",
    "# Last state\n",
    "print('\\nLast state:', M[0][-1])\n",
    "\n",
    "print(\"hello1\")\n",
    "\n",
    "# Actions\n",
    "print('= Action space (%i actions) =' % len(M[1]))\n",
    "for i in range(len(M[1])):\n",
    "    print(M[1][i]) \n",
    "\n",
    "print(\"hello2\")\n",
    "\n",
    "# Random action\n",
    "a = rand.randint(len(M[1]))\n",
    "print('\\nRandom action: a =', M[1][a])\n",
    "\n",
    "# Observations\n",
    "print('= Observation space (%i observations) =' % len(M[2]))\n",
    "print('\\nObservations:')\n",
    "for i in range(min(10, len(M[2]))):\n",
    "    print(M[2][i]) \n",
    "\n",
    "print('...')\n",
    "\n",
    "# Random observation\n",
    "z = rand.randint(len(M[2]))\n",
    "print('\\nRandom observation: z =', M[2][z])\n",
    "\n",
    "# Last state\n",
    "print('\\nLast observation:', M[2][-1])\n",
    "\n",
    "# Transition probabilities\n",
    "print('\\n= Transition probabilities =')\n",
    "\n",
    "for i in range(len(M[1])):\n",
    "    print('\\nTransition probability matrix dimensions (action %s):' % M[1][i], M[3][i].shape)\n",
    "    print('Dimensions add up for action \"%s\"?' % M[1][i], np.isclose(np.sum(M[3][i]), len(M[0])))\n",
    "    \n",
    "print('\\nState-action pair (%s, %s) transitions to state(s)' % (M[0][s], M[1][a]))\n",
    "print(\"s' in\", np.array(M[0])[np.where(M[3][a][s, :] > 0)])\n",
    "\n",
    "# Observation probabilities\n",
    "print('\\n= Observation probabilities =')\n",
    "\n",
    "for i in range(len(M[1])):\n",
    "    print('\\nObservation probability matrix dimensions (action %s):' % M[1][i], M[4][i].shape)\n",
    "    print('Dimensions add up for action \"%s\"?' % M[1][i], np.isclose(np.sum(M[4][i]), len(M[0])))\n",
    "    \n",
    "print('\\nState-action pair (%s, %s) yields observation(s)' % (M[0][s], M[1][a]))\n",
    "print(\"z in\", np.array(M[2])[np.where(M[4][a][s, :] > 0)])\n",
    "\n",
    "# Cost\n",
    "print('\\n= Costs =')\n",
    "\n",
    "print('\\nCost for the state-action pair (%s, %s):' % (M[0][s], M[1][a]))\n",
    "print('c(s, a) =', M[5][s, a])\n",
    "\n",
    "# Discount\n",
    "print('\\n= Discount =')\n",
    "print('\\ngamma =', M[6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide below an example of application of the function with the file `garbage-big.npz` that you can use as a first \"sanity check\" for your code. Note that, even fixing the seed, the results you obtain may slightly differ.\n",
    "\n",
    "```python\n",
    "import numpy.random as rand\n",
    "\n",
    "M = load_pomdp('garbage-big.npz', 0.99)\n",
    "\n",
    "rand.seed(42)\n",
    "\n",
    "# States\n",
    "print('= State space (%i states) =' % len(M[0]))\n",
    "print('\\nStates:')\n",
    "for i in range(min(10, len(M[0]))):\n",
    "    print(M[0][i]) \n",
    "\n",
    "print('...')\n",
    "\n",
    "# Random state\n",
    "s = rand.randint(len(M[0]))\n",
    "print('\\nRandom state: s =', M[0][s])\n",
    "\n",
    "# Last state\n",
    "print('\\nLast state:', M[0][-1])\n",
    "\n",
    "# Actions\n",
    "print('= Action space (%i actions) =' % len(M[1]))\n",
    "for i in range(len(M[1])):\n",
    "    print(M[1][i]) \n",
    "\n",
    "# Random action\n",
    "a = rand.randint(len(M[1]))\n",
    "print('\\nRandom action: a =', M[1][a])\n",
    "\n",
    "# Observations\n",
    "print('= Observation space (%i observations) =' % len(M[2]))\n",
    "print('\\nObservations:')\n",
    "for i in range(min(10, len(M[2]))):\n",
    "    print(M[2][i]) \n",
    "\n",
    "print('...')\n",
    "\n",
    "# Random observation\n",
    "z = rand.randint(len(M[2]))\n",
    "print('\\nRandom observation: z =', M[2][z])\n",
    "\n",
    "# Last state\n",
    "print('\\nLast observation:', M[2][-1])\n",
    "\n",
    "# Transition probabilities\n",
    "print('\\n= Transition probabilities =')\n",
    "\n",
    "for i in range(len(M[1])):\n",
    "    print('\\nTransition probability matrix dimensions (action %s):' % M[1][i], M[3][i].shape)\n",
    "    print('Dimensions add up for action \"%s\"?' % M[1][i], np.isclose(np.sum(M[3][i]), len(M[0])))\n",
    "    \n",
    "print('\\nState-action pair (%s, %s) transitions to state(s)' % (M[0][s], M[1][a]))\n",
    "print(\"s' in\", np.array(M[0])[np.where(M[3][a][s, :] > 0)])\n",
    "\n",
    "# Observation probabilities\n",
    "print('\\n= Observation probabilities =')\n",
    "\n",
    "for i in range(len(M[1])):\n",
    "    print('\\nObservation probability matrix dimensions (action %s):' % M[1][i], M[4][i].shape)\n",
    "    print('Dimensions add up for action \"%s\"?' % M[1][i], np.isclose(np.sum(M[4][i]), len(M[0])))\n",
    "    \n",
    "print('\\nState-action pair (%s, %s) yields observation(s)' % (M[0][s], M[1][a]))\n",
    "print(\"z in\", np.array(M[2])[np.where(M[4][a][s, :] > 0)])\n",
    "\n",
    "# Cost\n",
    "print('\\n= Costs =')\n",
    "\n",
    "print('\\nCost for the state-action pair (%s, %s):' % (M[0][s], M[1][a]))\n",
    "print('c(s, a) =', M[5][s, a])\n",
    "\n",
    "# Discount\n",
    "print('\\n= Discount =')\n",
    "print('\\ngamma =', M[6])\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "= State space (462 states) =\n",
    "\n",
    "States:\n",
    "(0, None, empty)\n",
    "(0, 1, empty)\n",
    "(0, 9, empty)\n",
    "(0, 10, empty)\n",
    "(0, 11, empty)\n",
    "(0, 18, empty)\n",
    "(0, 19, empty)\n",
    "(0, 20, empty)\n",
    "(0, 21, empty)\n",
    "(0, 23, empty)\n",
    "...\n",
    "\n",
    "Random state: s = (7, 28, empty)\n",
    "\n",
    "Last state: (32, None, loaded)\n",
    "= Action space (6 actions) =\n",
    "Up\n",
    "Down\n",
    "Left\n",
    "Right\n",
    "Pick\n",
    "Drop\n",
    "\n",
    "Random action: a = Right\n",
    "= Observation space (78 observations) =\n",
    "\n",
    "Observations:\n",
    "(8, no garbage, empty)\n",
    "(31, no garbage, loaded)\n",
    "(27, full, empty)\n",
    "(9, no garbage, loaded)\n",
    "(2, no garbage, loaded)\n",
    "(29, full, empty)\n",
    "(4, no garbage, empty)\n",
    "(23, no garbage, empty)\n",
    "(23, full, empty)\n",
    "(11, no garbage, loaded)\n",
    "...\n",
    "\n",
    "Random observation: z = (0, no garbage, loaded)\n",
    "\n",
    "Last observation: (12, no garbage, loaded)\n",
    "\n",
    "= Transition probabilities =\n",
    "\n",
    "Transition probability matrix dimensions (action Up): (462, 462)\n",
    "Dimensions add up for action \"Up\"? True\n",
    "\n",
    "Transition probability matrix dimensions (action Down): (462, 462)\n",
    "Dimensions add up for action \"Down\"? True\n",
    "\n",
    "Transition probability matrix dimensions (action Left): (462, 462)\n",
    "Dimensions add up for action \"Left\"? True\n",
    "\n",
    "Transition probability matrix dimensions (action Right): (462, 462)\n",
    "Dimensions add up for action \"Right\"? True\n",
    "\n",
    "Transition probability matrix dimensions (action Pick): (462, 462)\n",
    "Dimensions add up for action \"Pick\"? True\n",
    "\n",
    "Transition probability matrix dimensions (action Drop): (462, 462)\n",
    "Dimensions add up for action \"Drop\"? True\n",
    "\n",
    "State-action pair ((7, 28, empty), Right) transitions to state(s)\n",
    "s' in ['(8, 28, empty)']\n",
    "\n",
    "= Observation probabilities =\n",
    "\n",
    "Observation probability matrix dimensions (action Up): (462, 78)\n",
    "Dimensions add up for action \"Up\"? True\n",
    "\n",
    "Observation probability matrix dimensions (action Down): (462, 78)\n",
    "Dimensions add up for action \"Down\"? True\n",
    "\n",
    "Observation probability matrix dimensions (action Left): (462, 78)\n",
    "Dimensions add up for action \"Left\"? True\n",
    "\n",
    "Observation probability matrix dimensions (action Right): (462, 78)\n",
    "Dimensions add up for action \"Right\"? True\n",
    "\n",
    "Observation probability matrix dimensions (action Pick): (462, 78)\n",
    "Dimensions add up for action \"Pick\"? True\n",
    "\n",
    "Observation probability matrix dimensions (action Drop): (462, 78)\n",
    "Dimensions add up for action \"Drop\"? True\n",
    "\n",
    "State-action pair ((7, 28, empty), Right) yields observation(s)\n",
    "z in ['(7, no garbage, empty)']\n",
    "\n",
    "= Costs =\n",
    "\n",
    "Cost for the state-action pair ((7, 28, empty), Right):\n",
    "c(s, a) = 0.501\n",
    "\n",
    "= Discount =\n",
    "\n",
    "gamma = 0.99\n",
    "```\n",
    "\n",
    "**Note:** For debug purposes, we also provide a second file, `garbage-small.npz`, that contains a 6-state POMDP that you can use to verify if your results make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sampling\n",
    "\n",
    "You are now going to sample random trajectories of your POMDP and observe the impact it has on the corresponding belief."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 2.\n",
    "\n",
    "Write a function called `gen_trajectory` that generates a random POMDP trajectory using a uniformly random policy. Your function should receive, as input, a POMDP described as a tuple like that from **Activity 1** and two integers, `x0` and `n` and return a tuple with 3 elements, where:\n",
    "\n",
    "1. The first element is a `numpy` array corresponding to a sequence of `n + 1` state indices, $x_0,x_1,\\ldots,x_n$, visited by the agent when following a uniform policy (i.e., a policy where actions are selected uniformly at random) from state with index `x0`. In other words, you should select $x_1$ from $x_0$ using a random action; then $x_2$ from $x_1$, etc.\n",
    "2. The second element is a `numpy` array corresponding to the sequence of `n` action indices, $a_0,\\ldots,a_{n-1}$, used in the generation of the trajectory in 1.;\n",
    "3. The third element is a `numpy` array corresponding to the sequence of `n` observation indices, $z_1,\\ldots,z_n$, experienced by the agent during the trajectory in 1.\n",
    "\n",
    "The `numpy` array in 1. should have a shape `(n + 1,)`; the `numpy` arrays from 2. and 3. should have a shape `(n,)`.\n",
    "\n",
    "**Note:** Your function should work for **any** POMDP specified as above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T14:09:53.451420Z",
     "start_time": "2022-04-03T14:09:53.431531Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, using the POMDP from **Activity 1** you could obtain the following interaction.\n",
    "\n",
    "```python\n",
    "\n",
    "rand.seed(42)\n",
    "\n",
    "# Number of steps and initial state\n",
    "steps = 10\n",
    "s0    = 106 # State (18, 0, 2)\n",
    "\n",
    "# Generate trajectory\n",
    "t = gen_trajectory(M, s0, steps)\n",
    "\n",
    "# Check shapes\n",
    "print('Shape of state trajectory:', t[0].shape)\n",
    "print('Shape of state trajectory:', t[1].shape)\n",
    "print('Shape of state trajectory:', t[2].shape)\n",
    "\n",
    "# Print trajectory\n",
    "for i in range(steps):\n",
    "    print('\\n- Time step %i -' % i)\n",
    "    print('State:', M[0][t[0][i]], '(state %i)' % t[0][i])\n",
    "    print('Action selected:', M[1][t[1][i]], '(action %i)' % t[1][i])\n",
    "    print('Resulting state:', M[0][t[0][i+1]], '(state %i)' % t[0][i+1])\n",
    "    print('Observation:', M[2][t[2][i]], '(observation %i)' % t[2][i])\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Shape of state trajectory: (11,)\n",
    "Shape of state trajectory: (10,)\n",
    "Shape of state trajectory: (10,)\n",
    "\n",
    "- Time step 0 -\n",
    "State: (8, 9, empty) (state 106)\n",
    "Action selected: Right (action 3)\n",
    "Resulting state: (10, 9, empty) (state 132)\n",
    "Observation: (10, no garbage, empty) (observation 41)\n",
    "\n",
    "- Time step 1 -\n",
    "State: (10, 9, empty) (state 132)\n",
    "Action selected: Pick (action 4)\n",
    "Resulting state: (10, 9, empty) (state 132)\n",
    "Observation: (10, no garbage, empty) (observation 41)\n",
    "\n",
    "- Time step 2 -\n",
    "State: (10, 9, empty) (state 132)\n",
    "Action selected: Left (action 2)\n",
    "Resulting state: (8, 9, empty) (state 106)\n",
    "Observation: (8, no garbage, empty) (observation 0)\n",
    "\n",
    "- Time step 3 -\n",
    "State: (8, 9, empty) (state 106)\n",
    "Action selected: Left (action 2)\n",
    "Resulting state: (7, 9, empty) (state 93)\n",
    "Observation: (7, no garbage, empty) (observation 64)\n",
    "\n",
    "- Time step 4 -\n",
    "State: (7, 9, empty) (state 93)\n",
    "Action selected: Right (action 3)\n",
    "Resulting state: (8, 9, empty) (state 106)\n",
    "Observation: (8, no garbage, empty) (observation 0)\n",
    "\n",
    "- Time step 5 -\n",
    "State: (8, 9, empty) (state 106)\n",
    "Action selected: Right (action 3)\n",
    "Resulting state: (10, 9, empty) (state 132)\n",
    "Observation: (10, no garbage, empty) (observation 41)\n",
    "\n",
    "- Time step 6 -\n",
    "State: (10, 9, empty) (state 132)\n",
    "Action selected: Drop (action 5)\n",
    "Resulting state: (10, 9, empty) (state 132)\n",
    "Observation: (10, no garbage, empty) (observation 41)\n",
    "\n",
    "- Time step 7 -\n",
    "State: (10, 9, empty) (state 132)\n",
    "Action selected: Left (action 2)\n",
    "Resulting state: (8, 9, empty) (state 106)\n",
    "Observation: (8, no garbage, empty) (observation 0)\n",
    "\n",
    "- Time step 8 -\n",
    "State: (8, 9, empty) (state 106)\n",
    "Action selected: Right (action 3)\n",
    "Resulting state: (10, 9, empty) (state 132)\n",
    "Observation: (10, no garbage, empty) (observation 41)\n",
    "\n",
    "- Time step 9 -\n",
    "State: (10, 9, empty) (state 132)\n",
    "Action selected: Drop (action 5)\n",
    "Resulting state: (10, 9, empty) (state 132)\n",
    "Observation: (10, no garbage, empty) (observation 41)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now write a function that samples a given number of possible belief points for a POMDP. To do that, you will use the function from **Activity 2**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Activity 3.\n",
    "\n",
    "Write a function called `sample_beliefs` that receives, as input, a POMDP described as a tuple like that from **Activity 1** and an integer `n`, and return a tuple with `n + 1` elements **or less**, each corresponding to a possible belief state (represented as a $1\\times|\\mathcal{X}|$ vector). To do so, your function should\n",
    "\n",
    "* Generate a trajectory with `n` steps from a random initial state, using the function `gen_trajectory` from **Activity 2**.\n",
    "* For the generated trajectory, compute the corresponding sequence of beliefs, assuming that the agent does not know its initial state (i.e., the initial belief is the uniform belief, and should also be considered). \n",
    "\n",
    "Your function should return a tuple with the resulting beliefs, **ignoring duplicate beliefs or beliefs whose distance is smaller than $10^{-3}$.**\n",
    "\n",
    "**Suggestion:** You may want to define an auxiliary function `belief_update` that receives a POMDP, a belief, an action and an observation and returns the updated belief.\n",
    "\n",
    "**Note:** Your function should work for **any** POMDP specified as above. To compute the distance between vectors, you may find useful `numpy`'s function `linalg.norm`.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T14:09:53.536357Z",
     "start_time": "2022-04-03T14:09:53.455872Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, using the POMDP from **Activity 1** you could obtain the following interaction.\n",
    "\n",
    "```python\n",
    "rand.seed(42)\n",
    "\n",
    "# 3 sample beliefs + initial belief\n",
    "B = sample_beliefs(M, 3)\n",
    "print('%i beliefs sampled:' % len(B))\n",
    "for i in range(len(B)):\n",
    "    print(np.round(B[i], 3))\n",
    "    print('Belief adds to 1?', np.isclose(B[i].sum(), 1.))\n",
    "\n",
    "# 100 sample beliefs\n",
    "B = sample_beliefs(M, 100)\n",
    "print('%i beliefs sampled.' % len(B))\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "4 beliefs sampled:\n",
    "[[0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
    "  0.002 0.002 0.002 0.002 0.002 0.002]]\n",
    "Belief adds to 1? True\n",
    "[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.054 0.079 0.079 0.079\n",
    "  0.079 0.079 0.079 0.079 0.079 0.079 0.079 0.079 0.079 0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.   ]]\n",
    "Belief adds to 1? True\n",
    "[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.038 0.08  0.08  0.08\n",
    "  0.08  0.08  0.08  0.08  0.08  0.08  0.08  0.08  0.08  0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.   ]]\n",
    "Belief adds to 1? True\n",
    "[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.026 0.081 0.081 0.081 0.081\n",
    "  0.081 0.081 0.081 0.081 0.081 0.081 0.081 0.081 0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
    "  0.    0.    0.    0.    0.    0.   ]]\n",
    "Belief adds to 1? True\n",
    "40 beliefs sampled.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MDP-based heuristics\n",
    "\n",
    "In this section you are going to compare different heuristic approaches for POMDPs discussed in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 4\n",
    "\n",
    "Write a function `solve_mdp` that takes as input a POMDP represented as a tuple like that of **Activity 1** and returns a `numpy` array corresponding to the **optimal $Q$-function for the underlying MDP**. Stop the algorithm when the error between iterations is smaller than $10^{-8}$.\n",
    "\n",
    "**Note:** Your function should work for **any** POMDP specified as above. Feel free to reuse one of the functions you implemented in Lab 2 (for example, value iteration).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T14:09:53.575520Z",
     "start_time": "2022-04-03T14:09:53.538939Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, you can run the following code on the POMDP from **Activity 1**.\n",
    "\n",
    "```python\n",
    "Q = solve_mdp(M)\n",
    "\n",
    "s = 115 # State (8, 28, empty)\n",
    "print('\\nQ-values at state %s:' % M[0][s], np.round(Q[s, :], 3))\n",
    "print('Best action at state %s:' % M[0][s], M[1][np.argmin(Q[s, :])])\n",
    "\n",
    "s = 429 # (0, None, loaded)\n",
    "print('\\nQ-values at state %s:' % M[0][s], np.round(Q[s, :], 3))\n",
    "print('Best action at state %s:' % M[0][s], M[1][np.argmin(Q[s, :])])\n",
    "\n",
    "s = 239 # State (18, 18, empty)\n",
    "print('\\nQ-values at state %s:' % M[0][s], np.round(Q[s, :], 3))\n",
    "print('Best action at state %s:' % M[0][s], M[1][np.argmin(Q[s, :])])\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Q-values at state (8, 28, empty): [39.945 39.738 39.945 39.945 40.341 40.341]\n",
    "Best action at state (8, 28, empty): Down\n",
    "\n",
    "Q-values at state (0, None, loaded): [38.318 37.966 38.318 38.318 38.318 37.695]\n",
    "Best action at state (0, None, loaded): Drop\n",
    "\n",
    "Q-values at state (18, 18, empty): [38.422 38.803 38.803 38.803 38.185 38.803]\n",
    "Best action at state (18, 18, empty): Pick\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 5\n",
    "\n",
    "You will now test the different MDP heuristics discussed in class. To that purpose, write down a function that, given a belief vector and the solution for the underlying MDP, computes the action prescribed by each of the three MDP heuristics. In particular, you should write down a function named `get_heuristic_action` that receives, as inputs:\n",
    "\n",
    "* A belief state represented as a `numpy` array like those of **Activity 3**;\n",
    "* The optimal $Q$-function for an MDP (computed, for example, using the function `solve_mdp` from **Activity 4**);\n",
    "* A string that can be either `\"mls\"`, `\"av\"`, or `\"q-mdp\"`;\n",
    "\n",
    "Your function should return an integer corresponding to the index of the action prescribed by the heuristic indicated by the corresponding string, i.e., the most likely state heuristic for `\"mls\"`, the action voting heuristic for `\"av\"`, and the $Q$-MDP heuristic for `\"q-mdp\"`. *In all heuristics, ties should be broken randomly, i.e., when maximizing/minimizing, you should randomly select between all maximizers/minimizers*.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T14:09:53.631572Z",
     "start_time": "2022-04-03T14:09:53.578340Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if you run your function in the examples from **Activity 3** using the $Q$-function from **Activity 4**, you can observe the following interaction.\n",
    "\n",
    "```python\n",
    "rand.seed(42)\n",
    "\n",
    "for b in B[:10]:\n",
    "    \n",
    "    if np.all(b > 0):\n",
    "        print('Belief (approx.) uniform')\n",
    "    else:\n",
    "        initial = True\n",
    "\n",
    "        for i in range(len(M[0])):\n",
    "            if b[0, i] > 0:\n",
    "                if initial:\n",
    "                    initial = False\n",
    "                    print('Belief: [', M[0][i], ': %.3f' % np.round(b[0, i], 3), end='')\n",
    "                else:\n",
    "                    print(',', M[0][i], ': %.3f' % np.round(b[0, i], 3), end='')\n",
    "        print(']')\n",
    "\n",
    "    print('MLS action:', M[1][get_heuristic_action(b, Q, 'mls')], end='; ')\n",
    "    print('AV action:', M[1][get_heuristic_action(b, Q, 'av')], end='; ')\n",
    "    print('Q-MDP action:', M[1][get_heuristic_action(b, Q, 'q-mdp')])\n",
    "\n",
    "    print()\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Belief (approx.) uniform\n",
    "MLS action: Down; AV action: Right; Q-MDP action: Right\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.058, (27, 1, empty) : 0.086, (27, 9, empty) : 0.086, (27, 10, empty) : 0.086, (27, 11, empty) : 0.086, (27, 18, empty) : 0.086, (27, 19, empty) : 0.086, (27, 20, empty) : 0.086, (27, 21, empty) : 0.086, (27, 23, empty) : 0.086, (27, 28, empty) : 0.086, (27, 29, empty) : 0.086]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.041, (27, 1, empty) : 0.087, (27, 9, empty) : 0.087, (27, 10, empty) : 0.087, (27, 11, empty) : 0.087, (27, 18, empty) : 0.087, (27, 19, empty) : 0.087, (27, 20, empty) : 0.087, (27, 21, empty) : 0.087, (27, 23, empty) : 0.087, (27, 28, empty) : 0.087, (27, 29, empty) : 0.087]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.029, (27, 1, empty) : 0.088, (27, 9, empty) : 0.088, (27, 10, empty) : 0.088, (27, 11, empty) : 0.088, (27, 18, empty) : 0.088, (27, 19, empty) : 0.088, (27, 20, empty) : 0.088, (27, 21, empty) : 0.088, (27, 23, empty) : 0.088, (27, 28, empty) : 0.088, (27, 29, empty) : 0.088]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.020, (27, 1, empty) : 0.089, (27, 9, empty) : 0.089, (27, 10, empty) : 0.089, (27, 11, empty) : 0.089, (27, 18, empty) : 0.089, (27, 19, empty) : 0.089, (27, 20, empty) : 0.089, (27, 21, empty) : 0.089, (27, 23, empty) : 0.089, (27, 28, empty) : 0.089, (27, 29, empty) : 0.089]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.014, (27, 1, empty) : 0.090, (27, 9, empty) : 0.090, (27, 10, empty) : 0.090, (27, 11, empty) : 0.090, (27, 18, empty) : 0.090, (27, 19, empty) : 0.090, (27, 20, empty) : 0.090, (27, 21, empty) : 0.090, (27, 23, empty) : 0.090, (27, 28, empty) : 0.090, (27, 29, empty) : 0.090]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.010, (27, 1, empty) : 0.090, (27, 9, empty) : 0.090, (27, 10, empty) : 0.090, (27, 11, empty) : 0.090, (27, 18, empty) : 0.090, (27, 19, empty) : 0.090, (27, 20, empty) : 0.090, (27, 21, empty) : 0.090, (27, 23, empty) : 0.090, (27, 28, empty) : 0.090, (27, 29, empty) : 0.090]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.007, (27, 1, empty) : 0.090, (27, 9, empty) : 0.090, (27, 10, empty) : 0.090, (27, 11, empty) : 0.090, (27, 18, empty) : 0.090, (27, 19, empty) : 0.090, (27, 20, empty) : 0.090, (27, 21, empty) : 0.090, (27, 23, empty) : 0.090, (27, 28, empty) : 0.090, (27, 29, empty) : 0.090]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.005, (27, 1, empty) : 0.090, (27, 9, empty) : 0.090, (27, 10, empty) : 0.090, (27, 11, empty) : 0.090, (27, 18, empty) : 0.090, (27, 19, empty) : 0.090, (27, 20, empty) : 0.090, (27, 21, empty) : 0.090, (27, 23, empty) : 0.090, (27, 28, empty) : 0.090, (27, 29, empty) : 0.090]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.003, (27, 1, empty) : 0.091, (27, 9, empty) : 0.091, (27, 10, empty) : 0.091, (27, 11, empty) : 0.091, (27, 18, empty) : 0.091, (27, 19, empty) : 0.091, (27, 20, empty) : 0.091, (27, 21, empty) : 0.091, (27, 23, empty) : 0.091, (27, 28, empty) : 0.091, (27, 29, empty) : 0.091]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now implement the last heuristic, the \"Fast Informed Bound\" (or FIB) heuristic. To that purpose, you will write a function to compute the FIB Q-function.\n",
    "\n",
    "---\n",
    "\n",
    "#### Activity 6\n",
    "\n",
    "Write a function `solve_fib` that takes as input a POMDP represented as a tuple like that of **Activity 1** and returns a `numpy` array corresponding to the **FIB $Q$-function**, that verifies the recursion\n",
    "\n",
    "$$Q_{FIB}(x,a)=c(x,a)+\\gamma\\sum_{z\\in\\mathcal{Z}}\\min_{a'\\in\\mathcal{A}}\\sum_{x'\\in\\mathcal{X}}\\mathbf{P}(x'\\mid x,a)\\mathbf{O}(z\\mid x',a)Q_{FIB}(x',a').$$\n",
    "\n",
    "Stop the algorithm when the error between iterations is smaller than $10^{-1}$. Run the example code below to compare all the heuristics. What can you conclude from the results?\n",
    "\n",
    "**Note:** Your function should work for **any** POMDP specified as above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T14:09:53.713023Z",
     "start_time": "2022-04-03T14:09:53.634787Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Insert your comments here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function `solve_fib` in the function from `get_heuristic_action` from Activity 5 for the beliefs in the example from **Activity 3**, you can observe the following interaction.\n",
    "\n",
    "```python\n",
    "Qfib = solve_fib(M)\n",
    "\n",
    "s = 115 # State (8, 28, empty)\n",
    "print('\\nQ-values at state %s:' % M[0][s], np.round(Qfib[s, :], 3))\n",
    "print('Best action at state %s:' % M[0][s], M[1][np.argmin(Qfib[s, :])])\n",
    "\n",
    "s = 429 # (0, None, loaded)\n",
    "print('\\nQ-values at state %s:' % M[0][s], np.round(Qfib[s, :], 3))\n",
    "print('Best action at state %s:' % M[0][s], M[1][np.argmin(Qfib[s, :])])\n",
    "\n",
    "s = 239 # State (18, 18, empty)\n",
    "print('\\nQ-values at state %s:' % M[0][s], np.round(Qfib[s, :], 3))\n",
    "print('Best action at state %s:' % M[0][s], M[1][np.argmin(Qfib[s, :])])\n",
    "\n",
    "print()\n",
    "\n",
    "rand.seed(42)\n",
    "\n",
    "# Comparing the prescribed actions\n",
    "for b in B[:10]:\n",
    "    \n",
    "    if np.all(b > 0):\n",
    "        print('Belief (approx.) uniform')\n",
    "    else:\n",
    "        initial = True\n",
    "\n",
    "        for i in range(len(M[0])):\n",
    "            if b[0, i] > 0:\n",
    "                if initial:\n",
    "                    initial = False\n",
    "                    print('Belief: [', M[0][i], ': %.3f' % np.round(b[0, i], 3), end='')\n",
    "                else:\n",
    "                    print(',', M[0][i], ': %.3f' % np.round(b[0, i], 3), end='')\n",
    "        print(']')\n",
    "\n",
    "    print('MLS action:', M[1][get_heuristic_action(b, Q, 'mls')], end='; ')\n",
    "    print('AV action:', M[1][get_heuristic_action(b, Q, 'av')], end='; ')\n",
    "    print('Q-MDP action:', M[1][get_heuristic_action(b, Q, 'q-mdp')], end='; ')\n",
    "    print('FIB action:', M[1][get_heuristic_action(b, Qfib, 'q-mdp')])\n",
    "\n",
    "    print()\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Q-values at state (8, 28, empty): [39.876 39.673 39.876 39.876 40.274 40.274]\n",
    "Best action at state (8, 28, empty): Down\n",
    "\n",
    "Q-values at state (0, None, loaded): [38.281 37.927 38.281 38.281 38.281 37.659]\n",
    "Best action at state (0, None, loaded): Drop\n",
    "\n",
    "Q-values at state (18, 18, empty): [38.372 38.754 38.754 38.754 38.137 38.754]\n",
    "Best action at state (18, 18, empty): Pick\n",
    "\n",
    "Belief (approx.) uniform\n",
    "MLS action: Down; AV action: Right; Q-MDP action: Right; FIB action: Right\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.058, (27, 1, empty) : 0.086, (27, 9, empty) : 0.086, (27, 10, empty) : 0.086, (27, 11, empty) : 0.086, (27, 18, empty) : 0.086, (27, 19, empty) : 0.086, (27, 20, empty) : 0.086, (27, 21, empty) : 0.086, (27, 23, empty) : 0.086, (27, 28, empty) : 0.086, (27, 29, empty) : 0.086]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down; FIB action: Down\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.041, (27, 1, empty) : 0.087, (27, 9, empty) : 0.087, (27, 10, empty) : 0.087, (27, 11, empty) : 0.087, (27, 18, empty) : 0.087, (27, 19, empty) : 0.087, (27, 20, empty) : 0.087, (27, 21, empty) : 0.087, (27, 23, empty) : 0.087, (27, 28, empty) : 0.087, (27, 29, empty) : 0.087]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down; FIB action: Down\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.029, (27, 1, empty) : 0.088, (27, 9, empty) : 0.088, (27, 10, empty) : 0.088, (27, 11, empty) : 0.088, (27, 18, empty) : 0.088, (27, 19, empty) : 0.088, (27, 20, empty) : 0.088, (27, 21, empty) : 0.088, (27, 23, empty) : 0.088, (27, 28, empty) : 0.088, (27, 29, empty) : 0.088]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down; FIB action: Down\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.020, (27, 1, empty) : 0.089, (27, 9, empty) : 0.089, (27, 10, empty) : 0.089, (27, 11, empty) : 0.089, (27, 18, empty) : 0.089, (27, 19, empty) : 0.089, (27, 20, empty) : 0.089, (27, 21, empty) : 0.089, (27, 23, empty) : 0.089, (27, 28, empty) : 0.089, (27, 29, empty) : 0.089]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down; FIB action: Down\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.014, (27, 1, empty) : 0.090, (27, 9, empty) : 0.090, (27, 10, empty) : 0.090, (27, 11, empty) : 0.090, (27, 18, empty) : 0.090, (27, 19, empty) : 0.090, (27, 20, empty) : 0.090, (27, 21, empty) : 0.090, (27, 23, empty) : 0.090, (27, 28, empty) : 0.090, (27, 29, empty) : 0.090]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down; FIB action: Down\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.010, (27, 1, empty) : 0.090, (27, 9, empty) : 0.090, (27, 10, empty) : 0.090, (27, 11, empty) : 0.090, (27, 18, empty) : 0.090, (27, 19, empty) : 0.090, (27, 20, empty) : 0.090, (27, 21, empty) : 0.090, (27, 23, empty) : 0.090, (27, 28, empty) : 0.090, (27, 29, empty) : 0.090]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down; FIB action: Down\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.007, (27, 1, empty) : 0.090, (27, 9, empty) : 0.090, (27, 10, empty) : 0.090, (27, 11, empty) : 0.090, (27, 18, empty) : 0.090, (27, 19, empty) : 0.090, (27, 20, empty) : 0.090, (27, 21, empty) : 0.090, (27, 23, empty) : 0.090, (27, 28, empty) : 0.090, (27, 29, empty) : 0.090]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down; FIB action: Down\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.005, (27, 1, empty) : 0.090, (27, 9, empty) : 0.090, (27, 10, empty) : 0.090, (27, 11, empty) : 0.090, (27, 18, empty) : 0.090, (27, 19, empty) : 0.090, (27, 20, empty) : 0.090, (27, 21, empty) : 0.090, (27, 23, empty) : 0.090, (27, 28, empty) : 0.090, (27, 29, empty) : 0.090]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down; FIB action: Down\n",
    "\n",
    "Belief: [ (27, None, empty) : 0.003, (27, 1, empty) : 0.091, (27, 9, empty) : 0.091, (27, 10, empty) : 0.091, (27, 11, empty) : 0.091, (27, 18, empty) : 0.091, (27, 19, empty) : 0.091, (27, 20, empty) : 0.091, (27, 21, empty) : 0.091, (27, 23, empty) : 0.091, (27, 28, empty) : 0.091, (27, 29, empty) : 0.091]\n",
    "MLS action: Down; AV action: Down; Q-MDP action: Down; FIB action: Down\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
